{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This notebook needs pymc and arviz to be installed\n",
    "# in the used environment. Either install them by the method\n",
    "# of your choice or uncomment the code below and run it.\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install arviz pymc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescoring Using Bayesian and Frequentist Logistic Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "After the initial scoring of Peptide Spectrum Matches (PSM) by sage, additional information\n",
    "such as the deviation of the spectrum's ion mobility or retention time to those predicted for\n",
    "the peptide can be utilized to refine the scoring. This process is called rescoring.\n",
    "Here, a statistical model or machine learning approaches are trained to predict wether the peptide\n",
    "behind a PSM is a target or a decoy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "plt.style.use(\"bmh\")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# The features we want to use for rescoring\n",
    "\n",
    "features = [\n",
    "    \"score\",\n",
    "    \"delta_rt\",\n",
    "    \"delta_ims\",\n",
    "    \"cosine_similarity\",\n",
    "    \"delta_mass\",\n",
    "    \"rank\",\n",
    "    \"isotope_error\",\n",
    "    \"average_ppm\",\n",
    "    \"delta_next\",\n",
    "    \"delta_best\",\n",
    "    \"matched_peaks\",\n",
    "    \"longest_b\",\n",
    "    \"longest_y\",\n",
    "    \"longest_y_pct\",\n",
    "    \"missed_cleavages\",\n",
    "    \"matched_intensity_pct\",\n",
    "    \"poisson\",\n",
    "    \"charge\",\n",
    "    \"intensity_ms1\",\n",
    "    \"intensity_ms2\",\n",
    "    \"collision_energy\",\n",
    "    \"spectral_entropy_similarity\",\n",
    "    \"spectral_correlation_similarity_pearson\",\n",
    "    \"spectral_correlation_similarity_spearman\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "\n",
    "As input data we will use a downsampled PSM dataset provided in \"/data\".\n",
    "We will split the data roughly in half and use one half for training and the other to evaluate our rescoring models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_whole = pd.read_csv(\"./data/psm_data.csv\", usecols=features+[\"decoy\",\"match_idx\"]).assign(decoy = lambda x: x.decoy.astype(int))\n",
    "# shuffle data for train test split\n",
    "data_shuffled = data_whole.sample(frac=1, random_state=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split based on peptide sequence\n",
    "# we want to make sure that PSMs of the same peptide\n",
    "# do not occur in both train and test set \n",
    "\n",
    "size_data = data_shuffled.shape[0]\n",
    "\n",
    "match_idx_sizes = data_shuffled.groupby(\"match_idx\").size()/size_data\n",
    "match_idx_train = match_idx_sizes.where(match_idx_sizes.cumsum() <= 0.5).dropna().index\n",
    "match_idx_test = match_idx_sizes.where(match_idx_sizes.cumsum() > 0.5).dropna().index\n",
    "        \n",
    "data_train = data_shuffled[data_shuffled.match_idx.isin(match_idx_train)]\n",
    "data_test = data_shuffled[data_shuffled.match_idx.isin(match_idx_test)]\n",
    "\n",
    "y_train = data_train[\"decoy\"]\n",
    "X_train = data_train.loc[:, features]\n",
    "\n",
    "y_test = data_test[\"decoy\"]\n",
    "X_test = data_test.loc[:, features]\n",
    "\n",
    "# we will need the log odds for the prior of the logistic regression\n",
    "prob_decoy_train = data_train.decoy.value_counts(normalize=True).loc[1]\n",
    "log_odds_decoy_train = np.log(prob_decoy_train/(1-prob_decoy_train))\n",
    "prob_decoy_test = data_test.decoy.value_counts(normalize=True).loc[1]\n",
    "log_odds_decoy_test = np.log(prob_decoy_test/(1-prob_decoy_test))\n",
    "\n",
    "# Standardize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to see the initial score distribution (in the test set) of PSMs that come from\n",
    "target and decoy peptides, respectively. We use the negative hyperscore to make it more comparable to the\n",
    "logit values of the logistic regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_rescoring = (data_test.assign(score = lambda x: -x[\"score\"])\n",
    "                            .sort_values(\"score\")\n",
    "                            .assign(percent_decoy= lambda x: x[\"decoy\"].cumsum() / x[\"decoy\"].sum() * 100)\n",
    "                            .assign(count_target = lambda x: (1-x[\"decoy\"]).cumsum()))\n",
    "\n",
    "Fig, ax1 = plt.subplots(1,1, figsize=(5,5))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(before_rescoring[\"score\"], before_rescoring[\"percent_decoy\"], color=\"red\")\n",
    "sns.histplot(before_rescoring, x=\"score\", ax=ax1, hue=\"decoy\")\n",
    "\n",
    "ax2.set_ylabel(\"Percent Decoy [%]\")\n",
    "ax1.set_xlabel(\"-Score\")\n",
    "# get the last row where the percent decoy is below or equal to 1\n",
    "cutoff = before_rescoring.where(before_rescoring[\"percent_decoy\"] <= 1).dropna().tail(1)\n",
    "ax2.vlines(cutoff[\"score\"], 0, 100, color=\"black\", linestyle=\"--\")\n",
    "ax2.annotate(f\"{cutoff['count_target'].values[0]:.0f} non decoy\", (cutoff[\"score\"].values[0]-10, 101), color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that with the initial score at a FDR of 1% 10,236 PSM are selected.\n",
    "Now one could train a variety of statistical or machine learning models to better separate true\n",
    "hits with false positive hits. In this notebook, we will use a bayesian and a frequentist logistic regression\n",
    "model as examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Bayesian Model\n",
    "\n",
    "For formulation of the bayesian model the python library [PyMC](https://www.pymc.io/welcome.html) is used.\n",
    "The bayesian logistic regression can be described like this:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i &\\sim \\textrm{Bernoulli}\\left(p_i\\right)\\\\\n",
    "p_i &= \\frac{1}{1+\\exp{\\left(-\\left(\\alpha+\\beta x_i\\right)\\right)}}\\\\\n",
    "\\beta &\\sim \\textrm{Normal}\\left( \\mu_{\\beta} = 0, \\sigma_{\\beta} = 0.1 \\right)\\\\\n",
    "\\alpha &\\sim \\textrm{Normal}\\left( \\mu_{\\alpha} = \\log\\left(\\frac{p_\\textrm{decoy}}{\\left(1-p_\\textrm{decoy}\\right)}\\right), \\sigma_{\\alpha} = 0.1 \\right)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It can be shown, that the usage of a normal prior for the parameters\n",
    "of the bayesian logistic regression corresponds to the L2-regularization in\n",
    "the frequentist model. The regularization strength can be tuned by the standard deviation of the normal priors. With $\\sigma_b=0.1$ we will use a rather small standard deviation (i.e. high regularization)\n",
    "in the prior for $\\beta$. The mean $\\mu_{\\beta}$ of the prior for $\\beta$ is set to zero. This comes with the idea that all features could have both a positive and a negative impact on the \n",
    "probability of a PSM to be a decoy PSM. \n",
    "The mean $\\mu_{\\alpha}$ of the prior for $\\alpha$ is set to the log odds of the base probability of a PSM to be a decoy PSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"peptides\": X_train.index.values,\n",
    "                      \"features\": X_train.columns}) as logistic_model:\n",
    "    x_data = pm.Data(\"x_data\", X_train_scaled)\n",
    "    y_data = pm.Data(\"y_data\", y_train)\n",
    "    alpha = pm.Normal(\"alpha\", mu=log_odds_decoy_train, sigma=0.1)\n",
    "    beta = pm.Normal(\"beta\", mu=0 , sigma=0.1, shape=X_train_scaled.shape[1], dims=[\"features\"])\n",
    "\n",
    "    logit_p = pm.Deterministic(\"logit_p\", alpha + pm.math.dot(x_data, beta), dims=[\"peptides\"])\n",
    "    obs = pm.Bernoulli(\"obs\", logit_p=logit_p, observed=y_data, dims=[\"peptides\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the Posterior Distribution\n",
    "\n",
    "Once the model is defined, we can sample from the posterior distribution and\n",
    "use these samples to predict on new data by sampling from the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model as model:\n",
    "    idata = pm.sample(init=\"adapt_diag\") # sample from the posterior\n",
    "    pm.set_data({\"x_data\": X_test_scaled, \"y_data\": y_test}) # set the new (test) data\n",
    "    model.set_dim(\"peptides\", X_test_scaled.shape[0], X_test.index.values) # set the new dimensions and coords\n",
    "    model.set_dim(\"features\", X_test_scaled.shape[1], X_test.columns)\n",
    "    pm.sample_posterior_predictive(idata, var_names=[\"obs\",\"logit_p\"], predictions=True, extend_inferencedata=True) # sample from the posterior predictive\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then display the posterior and its sampling trace via the `plot_trace` function\n",
    "of the [arviz](https://python.arviz.org/en/stable/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, var_names=[\"alpha\", \"beta\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this plot is quite crowded, we could summarize posterior information using the\n",
    "mean and the bayesian central credible intervals (94%). We will focus on the\n",
    "$\\beta$ parameters, as these represent the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_bayesian = idata.posterior.beta.to_dataframe().reset_index()\n",
    "sns.barplot(data=feature_importances_bayesian, x=\"beta\", y=\"features\", errorbar=(\"pi\",94))\n",
    "plt.xlabel(\"Posterior Mean of Logistic Regression Coefficients\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importances (PyMC)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us have a look how many PSM will be selected after rescoring\n",
    "using a FDR of 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoy_peptides = data_test.loc[data_test[\"decoy\"]==1,:].index\n",
    "target_peptides = data_test.loc[data_test[\"decoy\"]==0,:].index\n",
    "posterior_decoy = idata.predictions.logit_p.sel(peptides=decoy_peptides).mean(dim=[\"chain\",\"draw\"]).values.flatten()\n",
    "posterior_target = idata.predictions.logit_p.sel(peptides=target_peptides).mean(dim=[\"chain\",\"draw\"]).values.flatten()\n",
    "\n",
    "after_rescoring_pymc = (pd.DataFrame({\"mean_logit_p\":   np.concatenate([posterior_decoy, posterior_target]),\n",
    "                                      \"decoy\":          np.concatenate([np.ones_like(posterior_decoy), np.zeros_like(posterior_target)])})\n",
    "                            .sort_values(\"mean_logit_p\")\n",
    "                            .assign(percent_decoy= lambda x: x[\"decoy\"].cumsum() / x[\"decoy\"].sum() * 100)\n",
    "                            .assign(count_target = lambda x: (1-x[\"decoy\"]).cumsum()))\n",
    "Fig, ax1 = plt.subplots(1,1, figsize=(5,5))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(after_rescoring_pymc[\"mean_logit_p\"], after_rescoring_pymc[\"percent_decoy\"], color=\"red\")\n",
    "sns.histplot(after_rescoring_pymc, x=\"mean_logit_p\", ax=ax1, hue=\"decoy\")\n",
    "\n",
    "cutoff = after_rescoring_pymc.where(after_rescoring_pymc[\"percent_decoy\"] <= 1).dropna().tail(1)\n",
    "ax2.vlines(cutoff[\"mean_logit_p\"].values[0], 0, 100, color=\"black\", linestyle=\"--\")\n",
    "ax2.annotate(f\"{cutoff['count_target'].values[0]:.0f} non decoy\", (cutoff[\"mean_logit_p\"].values[0]-2.5, 101), color=\"black\")\n",
    "\n",
    "ax1.set_xlabel(\"Mean Logit Probability\")\n",
    "sns.move_legend(ax1, \"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above illustrates how rescoring increases the number of PSMs that would be accepted, if a 1% FDR is applied. \n",
    "The plot also illustrates that the target distribution becomes bimodal, with a sharp peak around 0, that approximately matches\n",
    "the decoy distribution and a broader peak to its left. Hence the plot also explains the principle behind rescoring:\n",
    "It assumes that decoy PSMs have the same properties as PSMs coming from random hits in the target database and that a model \n",
    "trained to separate decoy hits from target hits also separates true hits from random hits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist Model\n",
    "\n",
    "For the frequentist logistic regression we will use the implementation of [scikit-learn](https://scikit-learn.org/stable/).\n",
    "As mentioned above the frequentist analogue to the normal priors is a L2-regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"l2\", C=0.01)\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "logit_p = clf.decision_function(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_rescoring_sklearn = (data_test.assign(logit_p_sklearn = logit_p)\n",
    "                            .sort_values(\"logit_p_sklearn\")\n",
    "                            .assign(percent_decoy= lambda x: x[\"decoy\"].cumsum() / x[\"decoy\"].sum() * 100)\n",
    "                            .assign(count_target = lambda x: (1-x[\"decoy\"]).cumsum()))\n",
    "\n",
    "Fig, ax1 = plt.subplots(1,1, figsize=(5,5))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "sns.histplot(after_rescoring_sklearn, x=\"logit_p_sklearn\", ax=ax1, hue=\"decoy\")\n",
    "ax2.plot(after_rescoring_sklearn[\"logit_p_sklearn\"], after_rescoring_sklearn[\"percent_decoy\"], color=\"red\")\n",
    "\n",
    "\n",
    "cutoff = after_rescoring_sklearn.where(after_rescoring_sklearn[\"percent_decoy\"] <= 1).dropna().tail(1)\n",
    "\n",
    "ax2.vlines(cutoff[\"logit_p_sklearn\"].values[0], 0, 100, color=\"black\", linestyle=\"--\")\n",
    "ax2.annotate(f\"{cutoff['count_target'].values[0]:.0f} non decoy\", (cutoff[\"logit_p_sklearn\"].values[0]-5, 101), color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_frequentist = pd.DataFrame({\"features\": X_test.columns, \"beta\": clf.coef_.flatten()})\n",
    "sns.barplot(data = feature_importances_frequentist, x=\"beta\", y=\"features\")\n",
    "plt.xlabel(\"Logistic Regression Coefficients (frequentist)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance (Sklearn)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_comparison = pd.concat([feature_importances_bayesian.loc[:,[\"features\",\"beta\"]].assign(method=\"bayesian\"),feature_importances_frequentist.assign(method=\"frequentist\")])\n",
    "sns.barplot(data = feature_importances_comparison, x=\"beta\", y=\"features\", hue=\"method\", errorbar=(\"pi\",94), order=feature_importances_frequentist.sort_values(\"beta\").features)\n",
    "plt.xlabel(\"Logistic Regression Coefficients\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance Comparison\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two plots above demonstrate that the coefficients of the frequentist logistic model correspond to the posterior mean of the bayesian model.\n",
    "However, the posterior samples of the bayesian approach provide additional information on the uncertainty of the model's inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aki_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
